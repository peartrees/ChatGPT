{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3UQJhQtO4q4LNGkagkr15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peartrees/ChatGPT/blob/main/alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCAYSam6Qzrp",
        "outputId": "6bbd48da-98c1-4b73-fe06-2e14a33c353f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'alpaca.cpp'...\n",
            "remote: Enumerating objects: 392, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 392 (delta 95), reused 95 (delta 95), pack-reused 287\u001b[K\n",
            "Receiving objects: 100% (392/392), 1.48 MiB | 4.55 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n",
            "/content/alpaca.cpp\n",
            "/content/alpaca.cpp/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/alpaca.cpp/build\n",
            "[ 20%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/ggml.c:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kquantize_row_q4_0\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/ggml.c:413:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kpp\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  413 |     uint8_t \u001b[01;35m\u001b[Kpp\u001b[m\u001b[K[QK/2];\n",
            "      |             \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/ggml.c:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kggml_vec_dot_q4_0\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/ggml.c:1417:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[KcountBlocks\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            " 1417 |     const size_t \u001b[01;35m\u001b[KcountBlocks\u001b[m\u001b[K = nb;\n",
            "      |                  \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 40%] \u001b[32m\u001b[1mLinking C static library libggml.a\u001b[0m\n",
            "[ 40%] Built target ggml\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/chat.dir/chat.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kbool llama_model_load(const string&, llama_model&, gpt_vocab&, int)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:249:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kn_ctx\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  249 |         const int \u001b[01;35m\u001b[Kn_ctx\u001b[m\u001b[K   = hparams.n_ctx;\n",
            "      |                   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:188:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kwtype2\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  188 |     const ggml_type \u001b[01;35m\u001b[Kwtype2\u001b[m\u001b[K = GGML_TYPE_F32;\n",
            "      |                     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kbool llama_eval(const llama_model&, int, int, const std::vector<int>&, std::vector<float>&, size_t&)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:551:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kd_key\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  551 |     const int \u001b[01;35m\u001b[Kd_key\u001b[m\u001b[K = n_embd/n_head;\n",
            "      |               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:964:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} and â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  964 |         if (\u001b[01;35m\u001b[Kembd_inp.size() <= input_consumed\u001b[m\u001b[K && !is_interacting) {\n",
            "      |             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:996:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} and â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  996 |             while (\u001b[01;35m\u001b[Kembd_inp.size() > input_consumed\u001b[m\u001b[K) {\n",
            "      |                    \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:1003:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} and â€˜\u001b[01m\u001b[Kint32_t\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            " 1003 |                 if (\u001b[01;35m\u001b[Kembd.size() > params.n_batch\u001b[m\u001b[K) {\n",
            "      |                     \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:1009:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} and â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            " 1009 |             if (!input_noecho && params.use_color && \u001b[01;35m\u001b[Kembd_inp.size() == input_consumed\u001b[m\u001b[K) {\n",
            "      |                                                      \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/chat.cpp:1024:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} and â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            " 1024 |         if (params.interactive && \u001b[01;35m\u001b[Kembd_inp.size() <= input_consumed\u001b[m\u001b[K) {\n",
            "      |                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/chat.dir/utils.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/utils.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kvoid gpt_print_usage(int, char**, const gpt_params&)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/utils.cpp:76:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter â€˜\u001b[01m\u001b[Kargc\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "   76 | void gpt_print_usage(\u001b[01;35m\u001b[Kint argc\u001b[m\u001b[K, char ** argv, const gpt_params & params) {\n",
            "      |                      \u001b[01;35m\u001b[K~~~~^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/utils.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<int> llama_tokenize(const gpt_vocab&, const string&, bool)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/content/alpaca.cpp/utils.cpp:292:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kmax_len\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  292 |         int \u001b[01;35m\u001b[Kmax_len\u001b[m\u001b[K = std::min(len - i, MAX_TOKEN_LEN);\n",
            "      |             \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable chat\u001b[0m\n",
            "[100%] Built target chat\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rupeshs/alpaca.cpp\n",
        "%cd /content/alpaca.cpp\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make chat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o ggml-alpaca-7b-q4.bin -C - https://gateway.estuary.tech/gw/ipfs/QmQ1bf2BTnYxq73MFJWu1B7bQ2UD6qG7D7YDCxhTndVkPC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Kmyro3Q6-g",
        "outputId": "9410be10-8063-4a06-918c-bb8638980957"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4017M  100 4017M    0     0  61.1M      0  0:01:05  0:01:05 --:--:-- 67.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdCK9m5cRTgr",
        "outputId": "97c38980-2854-4575-f368-fa0fef6601e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: seed = 1684403534\n",
            "llama_model_load: loading model from 'ggml-alpaca-7b-q4.bin' - please wait ...\n",
            "llama_model_load: ggml ctx size = 6065.34 MB\n",
            "llama_model_load: memory_size =  2048.00 MB, n_mem = 65536\n",
            "llama_model_load: loading model part 1/1 from 'ggml-alpaca-7b-q4.bin'\n",
            "llama_model_load: .................................... done\n",
            "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "sampling parameters: temp = 0.100000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n",
            "\n",
            "\n",
            "== Running in chat mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to LLaMA.\n",
            " - If you want to submit another line, end your input in '\\'.\n",
            "\u001b[33m\n",
            "> \u001b[1m\u001b[32mHello!\n",
            "\n",
            "\u001b[0mWelcome to our website, we are glad you could join us here today\u001b[0m\n",
            "> \u001b[1m\u001b[32m\u001b[0mThe answer is 1085479263. The number of seconds in a year (leap years excluded) since the Gregorian calendar was introduced on January 1, 1582 CE by Pope Gregory XIII and adopted for use worldwide except for Thailand until December 31st , 1904 is:\n",
            "(67*365 + 1)* (3+1/4) = 1085479263 seconds.\u001b[0m\n",
            "> \u001b[1m\u001b[32mwhat is the next day of Wednesday\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0mThe next day after a wednesday would be Thursday, unless it was leap year in which case there are two days between them and Friday comes before that. ðŸ™‚\u001b[0m\n",
            "> \u001b[1m\u001b[32m\u001b[0m\"I'm sorry.\" \"It won't happen again!\" #\n",
            "\"\"No worries! I understand, but it will never happen again,\" she said with a smile on her face and in her voice. ðŸ™‚\u001b[0m\n",
            "> \u001b[1m\u001b[32m\u001b[0m\"I am so sorry for my actions.\" \"Please forgive me!\" #\n",
            "\"\"Forgiveness is granted, but I will never forget what you did,\" she said with a stern look on her face and in the tone of voice. ðŸ™\u001b[0m\n",
            "> \u001b[1m\u001b[32m\u001b[0m\"I'm sorry.\" \"Please forgive me!\" #\"\"Forgiveness is granted, but I will never forget what you did,\" she said with a stern look on her face and in the tone of voice. ðŸ™\u001b[0m\n",
            "> \u001b[1m\u001b[32m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLbFTsoTRcKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
